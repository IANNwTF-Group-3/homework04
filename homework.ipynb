{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import datetime\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mnist from tensorflow_datasets\n",
    "mnist = tfds.load(\"mnist\", split =[\"train\",\"test\"], as_supervised=True)\n",
    "train_ds = mnist[0]\n",
    "val_ds = mnist[1]\n",
    "\n",
    "# write function to create the dataset that we want\n",
    "def preprocess(data, batch_size, subtask):\n",
    "    # FIRST STEP\n",
    "    # image should be float\n",
    "    data = data.map(lambda x, t: (tf.cast(x, float), t))\n",
    "    # image should be flattened\n",
    "    data = data.map(lambda x, t: (tf.reshape(x, (-1,)), t))\n",
    "    # image vector will here have values between -1 and 1\n",
    "    data = data.map(lambda x,t: ((x/128.)-1., t))\n",
    "    # we want to have two mnist images in each example\n",
    "    # this leads to a single example being ((x1,y1),(x2,y2))\n",
    "    zipped_ds = tf.data.Dataset.zip((data.shuffle(2000), \n",
    "                                     data.shuffle(2000)))\n",
    "\n",
    "\n",
    "    # SECOND STEP\n",
    "    # a + b >= 5 is a boolean classification -> one output perceptron, BinaryCrossEntropy loss function\n",
    "    # y_true: 0 oder 1, y_pred : [0,1]\n",
    "    # \n",
    "    # a - b = y choose a number for {-9, -8, ..., 8, 9} softmax from 19 possibilities, CategoricalCrossEntropy loss function\n",
    "    # y_true: one-hot-vector size 19, y_pred : one-hot-vector size 19 softmax\n",
    "\n",
    "    if (subtask == 1):\n",
    "        # target is 1 if value1 + value2 >= 5 else 0\n",
    "        zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1] + x2[1] >= 5))\n",
    "    elif (subtask == 2):\n",
    "        # target is value of label1 - label2\n",
    "        zipped_ds= zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1] - x2[1]))\n",
    "\n",
    "    # transforms into int32 targets\n",
    "    zipped_ds = zipped_ds.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32)))\n",
    "\n",
    "    # batch the dataset\n",
    "    zipped_ds = zipped_ds.batch(batch_size)\n",
    "    # prefetch\n",
    "    zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return zipped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model of our network\n",
    "class TwinMNISTModel(tf.keras.Model):\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, optimizer, subtask):\n",
    "        super().__init__()\n",
    "        # inherit functionality from parent class\n",
    "\n",
    "        # optimizer, loss function and metrics\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # layers to be used\n",
    "        # feed 28*28 pixels into this layer, spits out 0-9 one-hot-vector\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "\n",
    "        # feed in 0-9 one-hot-vector, spits out softmax representation\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\n",
    "        # a + b >= 5 ? gets in two softmax representations of a digit, spits out 0 = False, 1 = True\n",
    "        if subtask == 1:\n",
    "            self.subtask = 1\n",
    "            self.out_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "            self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "            self.metrics_list = [tf.keras.metrics.BinaryAccuracy(name=\"Accuracy\"),\n",
    "                                 tf.keras.metrics.Mean(name=\"loss\")]\n",
    "                                 \n",
    "        # a - b = y, gets in two softmax representations of a digit, spits out a value\n",
    "        else:\n",
    "            self.subtask = 2\n",
    "            self.out_layer = tf.keras.layers.Dense(1, activation=None)\n",
    "            self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "            self.metrics_list = [tf.keras.metrics.Accuracy(name=\"Accuracy\"),\n",
    "                                 tf.keras.metrics.Mean(name=\"loss\")]\n",
    "        \n",
    "        \n",
    "    # call method\n",
    "    @tf.function\n",
    "    def call(self, images, training=False):\n",
    "        # get images\n",
    "        img1, img2 = images\n",
    "\n",
    "        # let image go through first two layers, spits out one-hot-vector representing the number seen\n",
    "        img1_x = self.dense1(img1)\n",
    "        img1_x = self.dense2(img1_x)\n",
    "        \n",
    "        # same with second image\n",
    "        img2_x = self.dense1(img2)\n",
    "        img2_x = self.dense2(img2_x)\n",
    "\n",
    "        # concat the one-hot-vectors\n",
    "        combined_x = tf.concat([img1_x, img2_x ], axis=1)\n",
    "\n",
    "        # concatenated one hot vector into the output layer, squeeze the axis with dim=1\n",
    "        return tf.squeeze(self.out_layer(combined_x), axis=None, name=None)\n",
    "\n",
    "\n",
    "\n",
    "    # 3. metrics property\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "    # train_step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img1, img2, label = data\n",
    "        \n",
    "        # compute output and loss, train the variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self((img1, img2), training=True)\n",
    "            loss = self.loss_function(label, output)\n",
    "            \n",
    "        # update trainable variables\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # rounds output with bankers rounding\n",
    "        output = tf.map_fn(fn=lambda x: tf.round(x), elems=output)\n",
    "\n",
    "        # update metrics\n",
    "        self.metrics_list[0].update_state(output, label)\n",
    "        self.metrics_list[1].update_state(loss)\n",
    "        \n",
    "        # return a dict with metric information\n",
    "        return {m.name : m.result() for m in self.metrics_list}\n",
    "\n",
    "\n",
    "\n",
    "    # test_step method\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img1, img2, label = data\n",
    "\n",
    "        # compute output and loss, without training\n",
    "        output = self((img1, img2), training=False)\n",
    "        loss = self.loss_function(label, output)\n",
    "\n",
    "        output = tf.map_fn(fn=lambda x: tf.round(x), elems=output)\n",
    "        \n",
    "        # update metrics\n",
    "        self.metrics_list[0].update_state(output, label)\n",
    "        self.metrics_list[1].update_state(loss)\n",
    "\n",
    "        # return a dict with metric information \n",
    "        return {m.name : m.result() for m in self.metrics_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a summary writer to log data\n",
    "\n",
    "- use tf.summary.create_file_writer(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"Run-42\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs//{config_name}/{current_time}/val\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a training loop function\n",
    "\n",
    "Arguments: \n",
    " - the model to train, \n",
    " - the data to train on, \n",
    " - the data to test on, \n",
    " - how many epochs to train, \n",
    " - the train summary writer object to use for logging\n",
    " - the validation summary writer object to use for logging\n",
    " - a path to save trained model weights to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains the model by iterating through the dataset and applying training_step method epochs time\n",
    "def training_loop(model, train_ds, epochs, train_summary_writer):\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "\n",
    "        # train steps on all batches in the training data\n",
    "        for (img1, img2, label) in train_ds:\n",
    "            metrics = model.train_step((img1, img2, label))\n",
    "            \n",
    "            # keep data in summary with metrics\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics_list:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "        \n",
    "        # print current metric values and reset the metrics\n",
    "        print([f\"{key} : {value.numpy()}\" for (key, value ) in metrics.items()])\n",
    "        model.reset_metrics()\n",
    "\n",
    "\n",
    "\n",
    "# tests overall performance of model\n",
    "def test_loop(model, test_ds, val_summary_writer):\n",
    "    # test steps on every item in test dataset\n",
    "    for (img1, img2, label) in test_ds:\n",
    "        metrics = model.test_step((img1, img2, label))\n",
    "        \n",
    "        # keep data with metrics\n",
    "        with val_summary_writer.as_default():\n",
    "            for metric in model.metrics_list:\n",
    "                tf.summary.scalar(f\"{metric.name}\", metric.result(), step=1)\n",
    "    \n",
    "    #print current metric values and reset the metrics\n",
    "    print([f\"{key} : {value.numpy()}\" for (key, value ) in metrics.items()])\n",
    "    model.reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the training loop function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n",
      "testing beforehand subtask 1 ... SGD\n",
      "['Accuracy : 0.163100004196167', 'loss : 0.789199709892273']\n",
      "\n",
      "\n",
      "training subtask 1 ...  SGD\n",
      "epoch: 0\n",
      "['Accuracy : 0.8476999998092651', 'loss : 0.37372472882270813']\n",
      "epoch: 1\n",
      "['Accuracy : 0.9050499796867371', 'loss : 0.27001985907554626']\n",
      "epoch: 2\n",
      "['Accuracy : 0.9184666872024536', 'loss : 0.23780180513858795']\n",
      "\n",
      "\n",
      "testing subtask 1 ...  SGD\n",
      "['Accuracy : 0.9182000160217285', 'loss : 0.2278600037097931']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "testing beforehand subtask 1 ... ADAM\n",
      "['Accuracy : 0.17440000176429749', 'loss : 0.7313147783279419']\n",
      "\n",
      "\n",
      "training subtask 1 ...  ADAM\n",
      "\n",
      "epoch: 0\n",
      "['Accuracy : 0.8421833515167236', 'loss : 0.442692369222641']\n",
      "epoch: 1\n",
      "['Accuracy : 0.8412166833877563', 'loss : 0.43772614002227783']\n",
      "epoch: 2\n",
      "['Accuracy : 0.842033326625824', 'loss : 0.43633759021759033']\n",
      "\n",
      "\n",
      "testing subtask 1 ...  ADAM\n",
      "\n",
      "['Accuracy : 0.8379999995231628', 'loss : 0.44296377897262573']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "testing beforehand subtask 2 ...  SGD\n",
      "['Accuracy : 0.10119999945163727', 'loss : 16.841079711914062']\n",
      "\n",
      "\n",
      "training subtask 2 ...  SGD\n",
      "\n",
      "epoch: 0\n",
      "['Accuracy : 0.2733166813850403', 'loss : 4.365095615386963']\n",
      "epoch: 1\n",
      "['Accuracy : 0.4666999876499176', 'loss : 1.662474513053894']\n",
      "epoch: 2\n",
      "['Accuracy : 0.5738166570663452', 'loss : 1.2175707817077637']\n",
      "epoch: 3\n",
      "['Accuracy : 0.6350333094596863', 'loss : 0.9559168219566345']\n",
      "epoch: 4\n",
      "['Accuracy : 0.6714500188827515', 'loss : 0.8183249831199646']\n",
      "epoch: 5\n",
      "['Accuracy : 0.7111166715621948', 'loss : 0.7117191553115845']\n",
      "epoch: 6\n",
      "['Accuracy : 0.738183319568634', 'loss : 0.6151922345161438']\n",
      "epoch: 7\n",
      "['Accuracy : 0.7573333382606506', 'loss : 0.5525937080383301']\n",
      "epoch: 8\n",
      "['Accuracy : 0.773983359336853', 'loss : 0.49609458446502686']\n",
      "epoch: 9\n",
      "['Accuracy : 0.7897499799728394', 'loss : 0.449468731880188']\n",
      "epoch: 10\n",
      "['Accuracy : 0.8025166392326355', 'loss : 0.40582165122032166']\n",
      "epoch: 11\n",
      "['Accuracy : 0.8159499764442444', 'loss : 0.37639155983924866']\n",
      "epoch: 12\n",
      "['Accuracy : 0.8260499835014343', 'loss : 0.3354569971561432']\n",
      "epoch: 13\n",
      "['Accuracy : 0.831333339214325', 'loss : 0.32933008670806885']\n",
      "epoch: 14\n",
      "['Accuracy : 0.8446833491325378', 'loss : 0.29277387261390686']\n",
      "\n",
      "\n",
      "testing subtask 2...  SGD\n",
      "\n",
      "['Accuracy : 0.8149999976158142', 'loss : 0.8624987006187439']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "testing beforehand subtask 2 ...  ADAM\n",
      "['Accuracy : 0.10019999742507935', 'loss : 16.821243286132812']\n",
      "\n",
      "\n",
      "training subtask 2 ...  ADAM\n",
      "\n",
      "epoch: 0\n",
      "['Accuracy : 0.1315000057220459', 'loss : 9.89018726348877']\n",
      "epoch: 1\n",
      "['Accuracy : 0.22529999911785126', 'loss : 4.176091194152832']\n",
      "epoch: 2\n",
      "['Accuracy : 0.3508000075817108', 'loss : 2.197598457336426']\n",
      "epoch: 3\n",
      "['Accuracy : 0.4479166567325592', 'loss : 1.4996161460876465']\n",
      "epoch: 4\n",
      "['Accuracy : 0.5639500021934509', 'loss : 1.1854314804077148']\n",
      "epoch: 5\n",
      "['Accuracy : 0.6227166652679443', 'loss : 0.9947636127471924']\n",
      "epoch: 6\n",
      "['Accuracy : 0.6802499890327454', 'loss : 0.9142554402351379']\n",
      "epoch: 7\n",
      "['Accuracy : 0.7183666825294495', 'loss : 0.7900978922843933']\n",
      "epoch: 8\n",
      "['Accuracy : 0.7437999844551086', 'loss : 0.7252634763717651']\n",
      "epoch: 9\n",
      "['Accuracy : 0.7584666609764099', 'loss : 0.67400723695755']\n",
      "epoch: 10\n",
      "['Accuracy : 0.7642666697502136', 'loss : 0.658012866973877']\n",
      "epoch: 11\n",
      "['Accuracy : 0.7725333571434021', 'loss : 0.607478141784668']\n",
      "epoch: 12\n",
      "['Accuracy : 0.7817000150680542', 'loss : 0.5722076296806335']\n",
      "epoch: 13\n",
      "['Accuracy : 0.7859333157539368', 'loss : 0.5583285689353943']\n",
      "epoch: 14\n",
      "['Accuracy : 0.7936999797821045', 'loss : 0.5211049914360046']\n",
      "\n",
      "\n",
      "testing subtask 2...  ADAM\n",
      "\n",
      "['Accuracy : 0.7767999768257141', 'loss : 1.1241670846939087']\n"
     ]
    }
   ],
   "source": [
    "# get preprocessed data for both subtasks\n",
    "train_ds_sub1 = preprocess(train_ds, batch_size=32, subtask=1) #train_ds.apply(preprocess)\n",
    "val_ds_sub1 = preprocess(val_ds, batch_size=32, subtask=1) #val_ds.apply(preprocess)\n",
    "\n",
    "train_ds_sub2 = preprocess(train_ds, batch_size=32, subtask=2) #train_ds.apply(preprocess)\n",
    "val_ds_sub2 = preprocess(val_ds, batch_size=32, subtask=2) #val_ds.apply(preprocess)\n",
    "\n",
    "\n",
    "# check the contents of the dataset\n",
    "for img1, img2, label in train_ds_sub1.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "    #print(img1, img2, label)\n",
    "\n",
    "for img1, img2, label in train_ds_sub2.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "    #print(img1, img2, label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" SUBTASK 1\"\"\"\n",
    "model = TwinMNISTModel(tf.keras.optimizers.SGD(0.01), subtask=1)\n",
    "print(f\"testing beforehand subtask 1 ... SGD\")\n",
    "test_loop(model,\n",
    "        test_ds=val_ds_sub1,\n",
    "        val_summary_writer=val_summary_writer)\n",
    "\n",
    "print(f\"\\n\")\n",
    "        \n",
    "print(f\"training subtask 1 ...  SGD\")\n",
    "training_loop(model,\n",
    "              train_ds=train_ds_sub1,\n",
    "              epochs=3, \n",
    "              train_summary_writer=train_summary_writer)\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"testing subtask 1 ...  SGD\")\n",
    "test_loop(model,\n",
    "        test_ds=val_ds_sub1,\n",
    "        val_summary_writer=val_summary_writer)\n",
    "print(f\"\\n\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "\n",
    "\"\"\" SUBTASK 1\"\"\"\n",
    "model = TwinMNISTModel(tf.keras.optimizers.Adam(), subtask=1)\n",
    "print(f\"testing beforehand subtask 1 ... ADAM\")\n",
    "test_loop(model,\n",
    "        test_ds=val_ds_sub1,\n",
    "        val_summary_writer=val_summary_writer)\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"training subtask 1 ...  ADAM\\n\")\n",
    "training_loop(model,\n",
    "              train_ds=train_ds_sub1,\n",
    "              epochs=3, \n",
    "              train_summary_writer=train_summary_writer)\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"testing subtask 1 ...  ADAM\\n\")\n",
    "test_loop(model,\n",
    "        test_ds=val_ds_sub1,\n",
    "        val_summary_writer=val_summary_writer)\n",
    "print(f\"\\n\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "\n",
    "\"\"\" SUBTASK 2 \"\"\"\n",
    "model = TwinMNISTModel(tf.keras.optimizers.SGD(0.01), subtask=2)\n",
    "print(f\"testing beforehand subtask 2 ...  SGD\")\n",
    "test_loop(model,\n",
    "        test_ds=val_ds_sub2,\n",
    "        val_summary_writer=val_summary_writer)\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"training subtask 2 ...  SGD\\n\")\n",
    "training_loop(model,\n",
    "              train_ds=train_ds_sub2,\n",
    "              epochs=15, \n",
    "              train_summary_writer=train_summary_writer)\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"testing subtask 2...  SGD\\n\")\n",
    "test_loop(model,\n",
    "        test_ds=val_ds_sub2,\n",
    "        val_summary_writer=val_summary_writer)\n",
    "\n",
    "print(f\"\\n\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "\"\"\" SUBTASK 2 \"\"\"\n",
    "model = TwinMNISTModel(tf.keras.optimizers.Adam(), subtask=2)\n",
    "print(f\"testing beforehand subtask 2 ...  ADAM\")\n",
    "test_loop(model,\n",
    "        test_ds=val_ds_sub2,\n",
    "        val_summary_writer=val_summary_writer)\n",
    "        \n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"training subtask 2 ...  ADAM\\n\")\n",
    "training_loop(model,\n",
    "              train_ds=train_ds_sub2,\n",
    "              epochs=15, \n",
    "              train_summary_writer=train_summary_writer)\n",
    "\n",
    "print(f\"\\n\")\n",
    "\n",
    "print(f\"testing subtask 2...  ADAM\\n\")\n",
    "test_loop(model,\n",
    "        test_ds=val_ds_sub2,\n",
    "        val_summary_writer=val_summary_writer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-89494aa4f0a3f503\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-89494aa4f0a3f503\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# open the tensorboard logs\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b8c50c33561e7eff6eeea8f3e10c61ef76237379e0ac0cad7905faedae1c269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
