{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get mnist from tensorflow_datasets\n",
    "mnist = tfds.load(\"mnist\", split =[\"train\",\"test\"], as_supervised=True)\n",
    "train_ds = mnist[0]\n",
    "val_ds = mnist[1]\n",
    "\n",
    "# 2. write function to create the dataset that we want\n",
    "def preprocess(data, batch_size, subtask):\n",
    "    # FIRST STEP\n",
    "    # image should be float\n",
    "    data = data.map(lambda x, t: (tf.cast(x, float), t))\n",
    "    # image should be flattened\n",
    "    data = data.map(lambda x, t: (tf.reshape(x, (-1,)), t))\n",
    "    # image vector will here have values between -1 and 1\n",
    "    data = data.map(lambda x,t: ((x/128.)-1., t))\n",
    "    # we want to have two mnist images in each example\n",
    "    # this leads to a single example being ((x1,y1),(x2,y2))\n",
    "    zipped_ds = tf.data.Dataset.zip((data.shuffle(2000), \n",
    "                                     data.shuffle(2000)))\n",
    "\n",
    "\n",
    "    # SECOND STEP\n",
    "    # a + b >= 5 is a boolean classification -> one output perceptron, BinaryCrossEntropy loss function\n",
    "    # y_true: 0 oder 1, y_pred : [0,1]\n",
    "    # \n",
    "    # a - b = y choose a number for {-9, -8, ..., 8, 9} softmax from 19 possibilities, CategoricalCrossEntropy loss function\n",
    "    # y_true: one-hot-vector size 19, y_pred : one-hot-vector size 19 softmax\n",
    "\n",
    "    if (subtask == 1):\n",
    "        zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], x1[1] + x2[1] >= 5))\n",
    "        # transform boolean target to int\n",
    "        zipped_ds = zipped_ds.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32)))\n",
    "    elif (subtask == 2):\n",
    "        # target is -9, -8, ..., 8, 9\n",
    "        # enumerate 0 = -9, 1 = -8, .... , 18 = 9\n",
    "        zipped_ds= zipped_ds.map(lambda x1, x2: (x1[0], x2[0], tf.one_hot(x1[1] - x2[1]+9, depth = 19)))\n",
    "\n",
    "\n",
    "    # batch the dataset\n",
    "    zipped_ds = zipped_ds.batch(batch_size)\n",
    "    # prefetch\n",
    "    zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return zipped_ds\n",
    "\n",
    "train_ds_sub1 = preprocess(train_ds, batch_size=32, subtask=1) #train_ds.apply(preprocess)\n",
    "val_ds_sub1 = preprocess(val_ds, batch_size=32, subtask=1) #val_ds.apply(preprocess)\n",
    "\n",
    "train_ds_sub2 = preprocess(train_ds, batch_size=32, subtask=2) #train_ds.apply(preprocess)\n",
    "val_ds_sub2 = preprocess(val_ds, batch_size=32, subtask=2) #val_ds.apply(preprocess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32, 19)\n"
     ]
    }
   ],
   "source": [
    "# check the contents of the dataset\n",
    "for img1, img2, label in train_ds_sub1.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "    #print(img1, img2, label)\n",
    "\n",
    "for img1, img2, label in train_ds_sub2.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "    #print(img1, img2, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwinMNISTModel(tf.keras.Model):\n",
    "\n",
    "    # 1. constructor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # inherit functionality from parent class\n",
    "\n",
    "        # optimizer, loss function and metrics\n",
    "        #self.metrics_list = [tf.keras.metrics.BinaryAccuracy(),\n",
    "        #                    tf.keras.metrics.Mean(name=\"loss\")]\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        #self.optimizer = optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "        \n",
    "        self.loss_function_sub1 = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.loss_function_sub2 = tf.keras.losses.CategoricalCrossentropy()\n",
    "        \n",
    "        # layers to be used\n",
    "\n",
    "        # feed 28*28 pixels into this layer, spits out 0-9 one-hot-vector\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "\n",
    "        # feed in 0-9 one-hot-vector, spits out softmax representation\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "\n",
    "        # feed 0-9 one hot vector to represent number, spits out one-hot-vector where the 1 represents the number\n",
    "        #self.dense2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        #self.out_layer = tf.keras.layer.Dense(1,activation=tf.nn.sigmoid)\n",
    "\n",
    "        # gets in concatenated one-hot-vector with softmax activation, spits out prediction of subtask1 -> one perceptron with 0or1\n",
    "        self.out_layer_sub1 = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "        # gets in concatenated one-hot-vector with softmax activation, spits out prediction of subtask2 -> one-hot-vector of size 19\n",
    "        self.out_layer_sub2 = tf.keras.layers.Dense(19 ,activation=tf.nn.softmax)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # 2. call method (forward computation)\n",
    "    @tf.function\n",
    "    def call(self, images, subtask, training=False):\n",
    "        # get images\n",
    "        img1, img2 = images\n",
    "\n",
    "        # let image go through first two layers, spits out one-hot-vector representing the number seen\n",
    "        img1_x = self.dense1(img1)\n",
    "        img1_x = self.dense2(img1_x)\n",
    "        \n",
    "        # same with second image\n",
    "        img2_x = self.dense1(img2)\n",
    "        img2_x = self.dense2(img2_x)\n",
    "\n",
    "        # concat the one-hot-vectors\n",
    "        combined_x = tf.concat([img1_x, img2_x ], axis=1)\n",
    "\n",
    "        # feed concatenation into chosen subtask\n",
    "        if subtask == 1:   \n",
    "            ret = tf.squeeze(self.out_layer_sub1(combined_x), axis=None, name=None)\n",
    "        else:\n",
    "            ret = tf.squeeze(self.out_layer_sub2(combined_x), axis=None, name=None)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "\n",
    "    # 3. metrics property\n",
    "    #@property\n",
    "    #def metrics(self):\n",
    "    #    return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    #def reset_metrics(self):\n",
    "    #    for metric in self.metrics:\n",
    "    #        metrics.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "    # 5. train step method\n",
    "    def train_step(self, data, subtask):\n",
    "        img1, img2, label = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self((img1, img2), subtask, training=True)\n",
    "            if subtask == 1:\n",
    "                loss = self.loss_function_sub1(label, output)\n",
    "            else:\n",
    "                loss = self.loss_function_sub2(label, output)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return loss.numpy()\n",
    "\n",
    "\n",
    "    # 6. test_step method\n",
    "    def test_step(self, data, subtask):\n",
    "        img1, img2, label = data\n",
    "\n",
    "        prediction = self((img1, img2), subtask, training=False)\n",
    "\n",
    "        if subtask == 1:\n",
    "            loss = self.loss_function_sub1(label, prediction)\n",
    "        else:\n",
    "            loss = self.loss_function_sub2(label, prediction)\n",
    "            \n",
    "        return loss, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a summary writer to log data\n",
    "\n",
    "- use tf.summary.create_file_writer(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/\"\n",
    "\n",
    "val_log_path = f\"logs/\"\n",
    "\n",
    "train_summary_writer = ...\n",
    "\n",
    "val_summary_writer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a training loop function\n",
    "\n",
    "Arguments: \n",
    " - the model to train, \n",
    " - the data to train on, \n",
    " - the data to test on, \n",
    " - how many epochs to train, \n",
    " - the train summary writer object to use for logging\n",
    " - the validation summary writer object to use for logging\n",
    " - a path to save trained model weights to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_ds, subtask, epochs):\n",
    "    train_loss = 0\n",
    "    # 1. iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # 2. train steps on all batches in the training data\n",
    "        for (img1, img2, label) in train_ds:\n",
    "            train_loss += model.train_step((img1, img2, label), subtask=subtask)\n",
    "    return train_loss\n",
    "\n",
    "        # 3. log and print training metrics\n",
    "        #with train_summary_writer.as_default():\n",
    "        #    ...\n",
    "        # 4. reset metric objects\n",
    "        # 5. evaluate on validation data\n",
    "        # 6. log validation metrics\n",
    "        #with val_summary_writer.as_default():\n",
    "        #    ...\n",
    "        # 7. reset metric objects\n",
    "    # 8. save model weights\n",
    "\n",
    "def test_loop(model, test_ds, subtask):\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    ratio_agg = []\n",
    "\n",
    "    divisor = 0\n",
    "\n",
    "    for (img1, img2, label) in test_ds:\n",
    "        loss, prediction = model.test_step((img1, img2, label), subtask=subtask)\n",
    "        test_loss += loss\n",
    "\n",
    "        if subtask == 1:\n",
    "            acc_matrix = np.absolute(prediction.numpy() - label.numpy()) < 0.5\n",
    "            ratio = acc_matrix.sum() / acc_matrix.size\n",
    "            ratio_agg.append(ratio)\n",
    "        if subtask == 2:\n",
    "            sample_test_accuracy = np.argmax(label, axis=1) == np.argmax(prediction, axis=1)    \n",
    "            sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "            ratio_agg.append(sample_test_accuracy)\n",
    "\n",
    "            #sample_test_accuracy\n",
    "    \n",
    "    test_accuracy = np.average(np.array(ratio_agg))\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the training loop function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBTASK 1: \n",
      "\n",
      "Loss before training: 922.4093017578125, Accuracy before training: 0.0347444089456869\n",
      "\n",
      "training .....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open the tensorboard logs\n",
    "#%tensorboard --logdir logs/\n",
    "train_ds_sub1 = preprocess(train_ds, batch_size=32, subtask=1) #train_ds.apply(preprocess)\n",
    "val_ds_sub1 = preprocess(val_ds, batch_size=32, subtask=1) #val_ds.apply(preprocess)\n",
    "\n",
    "train_ds_sub2 = preprocess(train_ds, batch_size=32, subtask=2) #train_ds.apply(preprocess)\n",
    "val_ds_sub2 = preprocess(val_ds, batch_size=32, subtask=2) #val_ds.apply(preprocess)\n",
    "\n",
    "model = TwinMNISTModel()\n",
    "\n",
    "#print(test_loop(model, test_ds_sub1=val_ds_sub1))\n",
    "\n",
    "print(f\"SUBTASK 1: \\n\")\n",
    "\n",
    "loss, accuracy = test_loop(model, test_ds=val_ds_sub1, subtask=1)\n",
    "print(f\"Loss before training: {loss}, Accuracy before training: {accuracy}\\n\")\n",
    "\n",
    "print(f\"training .....\\n\")\n",
    "loss = training_loop(model, train_ds=train_ds_sub1, subtask=1, epochs=3)\n",
    "print(f\".....training done with loss: {loss}!\\n\\n\")\n",
    "\n",
    "loss, accuracy = test_loop(model, test_ds=val_ds_sub1, subtask=1)\n",
    "print(f\"Loss after training: {loss}, Accuracy after training: {accuracy}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"SUBTASK 2: \\n\")\n",
    "\n",
    "loss, accuracy = test_loop(model, test_ds=val_ds_sub2, subtask=2)\n",
    "print(f\"Loss before training: {loss}, Accuracy before training: {accuracy}\\n\")\n",
    "\n",
    "print(f\"training .....\\n\")\n",
    "loss = training_loop(model, train_ds=train_ds_sub2, subtask=2, epochs=3)\n",
    "print(f\".....training done with loss: {loss}!\\n\\n\")\n",
    "\n",
    "loss, accuracy = test_loop(model, test_ds=val_ds_sub2, subtask=2)\n",
    "print(f\"Loss after training: {loss}, Accuracy after training: {accuracy}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "training_loop() missing 3 required positional arguments: 'train_ds', 'subtask', and 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13283/3477615004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 2. pass arguments to training loop function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: training_loop() missing 3 required positional arguments: 'train_ds', 'subtask', and 'epochs'"
     ]
    }
   ],
   "source": [
    "# 1. instantiate model\n",
    "\n",
    "# 2. choose a path to save the weights\n",
    "\n",
    "save_path = ...\n",
    "\n",
    "# 2. pass arguments to training loop function\n",
    "\n",
    "training_loop(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b8c50c33561e7eff6eeea8f3e10c61ef76237379e0ac0cad7905faedae1c269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
